{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ed32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "from minimizers.minimize import GeneralLossMinimizer, CustomLossClassifier, CustomLossRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5fed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4f32f",
   "metadata": {},
   "source": [
    "---\n",
    "### Standard classification with  `GeneralLossMinimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de34f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9226666666666666\n",
      "Testing accuracy: 0.92\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', GeneralLossMinimizer(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_classes=2, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    flip_y=0., \n",
    "    n_clusters_per_class=1, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', GeneralLossMinimizer(random_state=seed)),\n",
    "])\n",
    "mod[-1].set_estimator_type('classifier')\n",
    "mod[-1].set_multi_output(False)\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train).round()\n",
    "print('Training accuracy:', (y_hat==y_train).mean())\n",
    "\n",
    "y_hat = mod.predict(X_test).round()\n",
    "print('Testing accuracy:', (y_hat==y_test).mean())\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1747c68",
   "metadata": {},
   "source": [
    "---\n",
    "### Standard regression with  `GeneralLossMinimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d911b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.29203808573391965\n",
      "Testing error: 0.2839428616952984\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', GeneralLossMinimizer(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(\n",
    "    n_targets=1, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', GeneralLossMinimizer(random_state=seed)),\n",
    "])\n",
    "mod[-1].set_estimator_type('regressor')\n",
    "mod[-1].set_multi_output(False)\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train).round()\n",
    "print('Training error:', ((y_train-y_hat)**2).mean()**0.5)\n",
    "\n",
    "y_hat = mod.predict(X_test).round()\n",
    "print('Testing error:', ((y_test-y_hat)**2).mean()**0.5)\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd88d6f",
   "metadata": {},
   "source": [
    "---\n",
    "### Binary classification with `CustomLossClassifier` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2467f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9226666666666666\n",
      "Testing accuracy: 0.92\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', CustomLossClassifier(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_classes=2, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    flip_y=0., \n",
    "    n_clusters_per_class=1, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', CustomLossClassifier(random_state=seed)),\n",
    "])\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train)\n",
    "print('Training accuracy:', (y_hat==y_train).mean())\n",
    "\n",
    "y_hat = mod.predict(X_test)\n",
    "print('Testing accuracy:', (y_hat==y_test).mean())\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8d1e1",
   "metadata": {},
   "source": [
    "---\n",
    "### Multi-categorical classification with `CustomLossClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b2c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.86\n",
      "Testing accuracy: 0.872\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', CustomLossClassifier(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_classes=3, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    flip_y=0., \n",
    "    n_clusters_per_class=1, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', CustomLossClassifier(random_state=seed)),\n",
    "])\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train)\n",
    "print('Training accuracy:', (y_hat==y_train).mean())\n",
    "\n",
    "y_hat = mod.predict(X_test)\n",
    "print('Testing accuracy:', (y_hat==y_test).mean())\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d8ce5",
   "metadata": {},
   "source": [
    "---\n",
    "### Single output `CustomLossRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb51d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 4.9928711244253936e-05\n",
      "Testing error: 5.395171147134042e-05\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', CustomLossRegressor(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(\n",
    "    n_targets=1, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', CustomLossRegressor(random_state=seed)),\n",
    "])\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train)\n",
    "print('Training error:', ((y_train-y_hat)**2).mean()**0.5)\n",
    "\n",
    "y_hat = mod.predict(X_test)\n",
    "print('Testing error:', ((y_test-y_hat)**2).mean()**0.5)\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97230a9d",
   "metadata": {},
   "source": [
    "---\n",
    "### Multi-output regression with `CustomLossRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e64b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 3.618715002718675e-05\n",
      "Testing error: 3.7006844356435374e-05\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf', CustomLossRegressor(random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(\n",
    "    n_targets=2, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    random_state=seed,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('clf', CustomLossRegressor(random_state=seed)),\n",
    "])\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train)\n",
    "print('Training error:', ((y_train-y_hat)**2).mean(1).mean()**0.5)\n",
    "\n",
    "y_hat = mod.predict(X_test)\n",
    "print('Testing error:', ((y_test-y_hat)**2).mean(1).mean()**0.5)\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d8558",
   "metadata": {},
   "source": [
    "---\n",
    "### Squared hinge loss with  `GeneralLossMinimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174da36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9226666666666666\n",
      "Testing accuracy: 0.928\n",
      "Pipeline(steps=[('sc', StandardScaler()),\n",
      "                ('clf',\n",
      "                 GeneralLossMinimizer(link_fn=<function linear_link at 0x106d01fc0>,\n",
      "                                      loss_fn=<function squared_hinge_loss at 0x106d01c60>,\n",
      "                                      random_state=0))])\n"
     ]
    }
   ],
   "source": [
    "def squared_hinge_loss(y, y_hat):\n",
    "\n",
    "    zeros = np.zeros_like(y)\n",
    "    margin = 1 - (y*y_hat)\n",
    "\n",
    "    return np.c_[zeros, margin].max(1) ** 2\n",
    "\n",
    "def linear_link(X, b):\n",
    "    \n",
    "    return X.dot(b)\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, \n",
    "    n_samples=1000, \n",
    "    n_features=10, \n",
    "    flip_y=0., \n",
    "    n_clusters_per_class=1, \n",
    "    random_state=seed,\n",
    ")\n",
    "y = np.where(y==1, 1, -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('sc', StandardScaler(\n",
    "    )),\n",
    "    ('clf', GeneralLossMinimizer(\n",
    "        random_state=seed, \n",
    "        loss_fn=squared_hinge_loss, \n",
    "        link_fn=linear_link,\n",
    "    )),\n",
    "])\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "y_hat = mod.predict(X_train)\n",
    "y_hat = np.where(y_hat>0, 1, -1)\n",
    "print('Training accuracy:', (y_hat==y_train).mean())\n",
    "\n",
    "y_hat = mod.predict(X_test)\n",
    "y_hat = np.where(y_hat>0, 1, -1)\n",
    "print('Testing accuracy:', (y_hat==y_test).mean())\n",
    "\n",
    "print(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee677a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnzr",
   "language": "python",
   "name": "mnzr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
